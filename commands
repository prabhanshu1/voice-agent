
# Setting up 
sudo add-apt-repository ppa:deadsnakes/ppa -y
sudo apt update && sudo apt upgrade -y
sudo apt install git pip gcc uvicorn ffmpeg espeak-ng python3.10 python3.10-venv python3.10-dev -y
sudo apt install nvidia-utils-550 nvidia-cuda-toolkit

# sudo apt --fix-broken install
# sudo apt-get remove --purge '^nvidia-.*'
# sudo apt-get autoremove -y
# sudo add-apt-repository ppa:graphics-drivers/ppa -y
# sudo apt update
# sudo apt install nvidia-driver-550 -y

# if nvidia cuda still errors - sudo apt install nvidia-utils-470-server nvidia-driver-535 -y
sudo reboot

# nvidia-cuda-toolkit if sudo apt install nvidia-cuda-toolkit is taking time/error
# wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda_12.8.1_570.124.06_linux.run
# sudo sh cuda_12.8.1_570.124.06_linux.run


#pip install fastapi uvicorn


# Whisper STT
python3.10 -m venv .venvSTT && source .venvSTT/bin/activate

git clone https://github.com/prabhanshu1/stt-local.git
# requirements for faster_whisper
pip install -r stt-local/requirements.txt
cd stt-local && uvicorn whisper_server:app --host 0.0.0.0 --port 8000

# whisper.cpp - for faster STT
# https://github.com/ggml-org/whisper.cpp?tab=readme-ov-file
#https://github.com/ggml-org/whisper.cpp/tree/master/examples/server
sh ./models/download-ggml-model.sh large-v3-turbo
./whisper.cpp/build/bin/whisper-server -m whisper.cpp/models/ggml-large-v3-turbo.bin --host 0.0.0.0 --port 8000 -t $(nproc) -ps -pc -pr -l en
# https://pypi.org/project/whisper-cpp-python/

# commands
# -l auto



# TTS 
python3.10 -m venv .venvTTS && source .venvTTS/bin/activate
git clone https://github.com/prabhanshu1/local-TTS.git
pip install -r local-TTS/requirements.txt
# pip install TTS
# tts-server --use_cuda true
#git file lownload
cd local-TTS && uvicorn tts:app --host 0.0.0.0 --port 5002

# Get list of speakers
## from TTS.api import TTS
# tts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts", progress_bar=False, gpu=True)
# print(tts.speakers)
#your_tts
## Speakers list : ['female-en-5', 'female-en-5\n', 'female-pt-4\n', 'male-en-2', 'male-en-2\n', 'male-pt-3\n']




# if getting error of torch 2.6.0 for using xtts
# pip uninstall torch
# pip install torch==2.5.1
# pip install torchaudio==2.5.1


# local LLM through Flask
python3.10 -m venv .venvLLM && source .venvLLM/bin/activate
git clone https://github.com/prabhanshu1/llm-local.git
pip install -r llm-local/requirements.txt
pip install llama-cpp-python --extra-index-url=https://abetlen.github.io/llama-cpp-python-cuBLAS-wheels/cu12.4 #last is cuda version

CMAKE_ARGS="-DLLAMA_CUDA=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --force-reinstall

# python llm-local/llm.py
uvicorn llm:app --host 0.0.0.0 --port 5000 --workers 1

# llama-cpp



# llama-cpp with cpp based server - for fast response
sudo apt install -y cmake build-essential libcurl4-openssl-dev

# https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cuda  : Building llama.cpp
git clone https://github.com/ggml-org/llama.cpp && cd llama.cpp

#Compilation
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release -- -j$(nproc) # last one to use multi-threading

python3.10 -m venv .venvLLAMACPP && source .venvLLAMACPP/bin/activate
pip install huggingface_hub

huggingface-cli download google/gemma-3-4b-it-qat-q4_0-gguf --local-dir ~/models/gemma --local-dir-use-symlinks False

./llama.cpp/build/bin/llama-server -m ~/models/gemma/gemma-3-4b-it-q4_0.gguf -ngl 100 -c 2048 -b 2048 --no-webui --mlock --threads $(nproc) --chat-template gemma --host 0.0.0.0 --port 8080

# -v Set verbosity level to infinity
# -lv Set the verbosity threshold. Messages with a higher verbosity will be ignored.
# --log-timestamps
# -c 2048 # context length
# -n 64 number of tokens to predict (default: -1, -1 = infinity) => use post
# --mlock                                 force system to keep model in RAM rather than swapping or compressing
# --temp N                                temperature (default: 0.8)
# --no-webui
# --chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's
#                                         metadata)
#                                         if suffix/prefix are specified, template will be disabled
#                                         only commonly used templates are accepted (unless --jinja is set
#                                         before this flag):
#                                         list of built-in templates:
#                                         bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,
#                                         deepseek3, exaone3, falcon3, gemma, gigachat, glmedge, granite,
#                                         llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4,
#                                         megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,
#                                         mistral-v7, monarch, openchat, orion, phi3, phi4, rwkv-world, vicuna,
#                                         vicuna-orca, yandex, zephyr




#https://github.com/ggml-org/llama.cpp/tree/master/examples/server


# Gated repos
git config --global credential.helper store
huggingface-cli login # enter token from huggingface




# install ollama and serve
curl -fsSL https://ollama.com/install.sh | sh

ollama pull gemma3:4b
sudo systemctl stop ollama

OLLAMA_HOST=0.0.0.0 OLLAMA_PORT=11434 ollama serve
# ollama serve # if ollama is not run with above command


# Livekit agent
curl -sSL https://get.livekit.io/cli | bash  # cli
lk token create \
  --api-key devkey --api-secret secret \
  --join --room test_room --identity test_user \
  --valid-for 24h

python3.10 -m venv .venvVoiceAgent && source .venvVoiceAgent/bin/activate
git clone https://github.com/prabhanshu1/voice-agent.git
cd voice-agent && pip install -r requirements.txt
pip install \
  "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0" \
  "livekit-plugins-noise-cancellation~=0.2" \
  "python-dotenv"

python backend/agent.py download-files
python backen/agent.py dev

#Monitoring
htop 
nvidia-smi -l 1 


# Tmux session - auto-savign and restoring even after server reboots
git clone https://github.com/tmux-plugins/tmux-resurrect ~/.tmux/plugins/tmux-resurrect
git clone https://github.com/tmux-plugins/tmux-continuum ~/.tmux/plugins/tmux-continuum
run-shell ~/.tmux/plugins/tmux-resurrect/resurrect.tmux
run-shell ~/.tmux/plugins/tmux-continuum/continuum.tmux
set -g @continuum-restore 'on'
set -g @resurrect-capture-pane-contents 'on'



# running an instance:
source .venvSTT/bin/activate && cd stt-local && uvicorn whisper_server:app --host 0.0.0.0 --port 8000

source .venvLLM/bin/activate && cd llm-local && uvicorn llm:app --host 0.0.0.0 --port 5000 --workers 1


source .venvTTS/bin/activate && tts-server --use_cuda true


#SIP#
lk sip inbound create inbound-trunk.json \
  --url http://localhost:7880 \
 --api-key devkey --api-secret secret